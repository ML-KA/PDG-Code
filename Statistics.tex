\section{Statistics}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\subsection{Probability}
TODO: Probability (general + simple), CDF, Variance, $L_p$-Space for Random-Variables, Jensens-Inequality for Random Variables, Fisher-Information etc.

\subsection{Distributions}
TODO: All the usual suspects, including observed

\subsection{Estimation}
TODO: ML, Score-Function, biased/unbiased, Cramér–Rao bound, confidence-interval

\subsection{Divergences}
Conventions for this section: $P$ and $Q$ are probability measures over a set $X$, and $P$ is absolutely continuous with respect to $Q$. $S$ is a space of all probability distributions with common support.
\subsubsection{Divergence}
A divergence on $S$ is a function $D: S \times S \rightarrow R$ satisfying
\begin{enumerate}
	\item $D(p || q) \geq 0  \forall p, q \in S$,
	\item $D(p || q) = 0 \Leftrightarrow p = q$
\end{enumerate}
\textit{A divergence is a "sense" of distance between two probability distributions. It's not a metric, but a pre-metric.}\\
\href{https://en.wikipedia.org/wiki/Divergence_(statistics)}{wikipedia}

\subsubsection{f-Divergence}
\begin{enumerate}
	\item Generalization of whole family of divergences
	\item For a convex function $f$ such that $f(1) = 0$, the f-divergence of $P$ from $Q$ is defined as:\\
	$D_{f}(P\parallel Q)\equiv \int _{{\Omega }}f\left({\frac{dP}{dQ}}\right)\,dQ$
	\item \href{https://en.wikipedia.org/wiki/Divergence_(statistics)}{wikipedia}
\end{enumerate}

\subsubsection{KL-Divergence}
\begin{enumerate}
	\item The Kullback–Leibler divergence from $Q$ to $P$ is defined as\\
	$D_{\mathrm {KL} }(P\|Q)=\int _{X}\log {\frac {dP}{dQ}}\,dP=D_{t\log t}$.
	\item maxmizing likelihood is equivalent to minimizing $D_{KL}(P(. \vert \theta^{\ast}) \, \Vert \, P(. \vert \theta))$, where $P(. \vert \theta^{\ast})$ is the true distribution and $P(. \vert \theta)$ is our estimate.
	\item \href{https://en.wikipedia.org/wiki/Kullback–Leibler_divergence}{wikipedia}
\end{enumerate}

\subsubsection{Jensen–Shannon divergence}
The Jensen–Shannon divergence from $Q$ to $P$ is defined as
\begin{align*}
	{{\rm {JSD}}}(P\parallel Q)={\frac  {1}{2}}D(P\parallel M)+{\frac  {1}{2}}D(Q\parallel M)
\end{align*},\\
 where $M={\frac  {1}{2}}(P+Q)$\\
\href{https://en.wikipedia.org/wiki/Jensen–Shannon_divergence}{wikipedia}

TODO: Wasserstein \& Wasserstein Dual