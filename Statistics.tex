% \begin{savequote}[75mm]
% Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
% \qauthor{Quoteauthor Lastname}
% \end{savequote}

\chapter{Statistics}
Statistics is a branch of mathematics dealing with the collection, organization, analysis, interpretation and presentation of data.
\href{https://en.wikipedia.org/wiki/Statistics}{wikipedia}

\section{Probability}
TODO: Probability (general + simple), CDF, Variance, Markov-Property, stationarity, etc.


\subsection{Statistic}
A statistic is a function of a sample where the function itself is independent of the sample's distribution; that is, the function can be stated before realization of the data. The term statistic is used both for the function and for the value of the function on a given sample.
\href{https://en.wikipedia.org/wiki/Statistic}{wikipedia}


\subsection{$L_p$-Space for Random-Variables}
The $L_p$-Norm for Random-Variables $X$, where $\mathbb{E}|X|^p < \infty$, is defined through:
\begin{align*}
	||X||_p:=(\mathbb{E}[|X|^p])^{\frac{1}{p}}
\end{align*}
\href{http://www2.stat.duke.edu/courses/Fall18/sta711/lec/wk-05.pdf}{lecture}

\subsection{Jensens-Inequality for Random Variables}
If $\phi$ is a konvex function and $X$ a Random-Variable, then
\begin{align*}
	\phi(\mathbb{E}X) \leq \mathbb{E}\phi(X)
\end{align*}
\href{https://en.wikipedia.org/wiki/Jensen%27s_inequality}{wikipedia}

\subsection{Fisher-Information}
For the parametric family $\mathcal{P} \in \{ \mathcal{P}_\theta | \theta \in \Theta_L\}$...TODO\\
If we assume $p_\theta(x,y)=p_\theta(y|x)p(x)$, the Fisher-Information Matrix $\mathcal{I}(\theta)$ becomes:
\begin{align*}
	\mathcal{I}(\theta) = \mathbb{E}_{(X,Y)\sim {P}_\theta}[\nabla_{\theta}\log p_{\theta}(Y|X)\otimes\nabla_{\theta}\log p_{\theta}(Y|X)]\text{,}
\end{align*}
where $\otimes$ is the inner-product.\\
\href{https://arxiv.org/abs/1711.01530}{paper}

\section{Distributions}
In this section, $X$ denotes a Random Vairable and $f$ the density-function.\\
TODO: More common distributions
\subsection{Normal Distribution}
If $X \sim {\mathcal {N}}(\mu ,\sigma ^{2})$ for ${\displaystyle \mu \in \mathbb {R}}$ and $\sigma ^{2} > 0 \in \mathbb {R}$, then:
\begin{align*}
	f(x) &= {\displaystyle {\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}\\
	\mathbb{E}X &= \mu \\
	Var[X] &= \sigma^2
\end{align*}
\href{https://en.wikipedia.org/wiki/Normal_distribution}{wikipedia}

\subsection{Normal Distribution (Multivariate)}
If $X \sim {\mathcal {N}}(\mu ,\Sigma)$ for $\mu \in \mathbb {R}^k$ and $\Sigma \in \mathbb {R}^{k \times k}$ with $\Sigma$ being positve semi-definite, then:
\begin{align*}
	f(x) &= \operatorname {det} (2\pi {\boldsymbol {\Sigma }})^{-{\frac {1}{2}}}\,e^{-{\frac {1}{2}}(\mathbf {x} -{\boldsymbol {\mu }})'{\boldsymbol {\Sigma }}^{-1}(\mathbf {x} -{\boldsymbol {\mu }})}\\
	\mathbb{E}X &= \mu \\
	Var[X] &= \Sigma
\end{align*}
\href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}{wikipedia}

\subsection{Empirical Distribution}
For any observation $X'=(x'_1, \cdots, x'_n)$, the empirical distribution is defined as:
\begin{align*}
	f(x) &= \hat{f}(x) =\frac{1}{n}\sum_{i=1}^{n}\delta(x - x_i)\text{, where $\delta$ is the dirac-delta function}\\
	\mathbb{E}X &= \hat{\mathbb{E}}X = \frac{1}{n}\sum_{i=1}^{n}x_i \\
	Var[X] &= \hat{Var}[X] =\frac{1}{n}\sum_{i=1}^{n}(x_i-\hat{\mathbb{E}}X)^2
\end{align*}
\href{http://www.stat.umn.edu/geyer/5102/slides/s1.pdf}{lecture}

\section{Estimation}
TODO: biased/unbiased, consistent, sufficient, Cramér–Rao bound, confidence-interval

\subsection{Estimator}
Examples:
\begin{enumerate}
	\item \textit{point estimation:}\\ The application of a point estimator (a statistic) to the data to obtain a point estimate. In Machine-Learning, estimating the parameters of neural networks is usually done via (a multidimensional) point-estimation. \href{https://en.wikipedia.org/wiki/Point_estimation}{wikipedia}
	\item \textit{Interval estimation:}\\ interval estimation is the use of sample data to calculate an interval of plausible values of an unknown population parameter.\href{https://en.wikipedia.org/wiki/Interval_estimation}{wikipedia}
	\item \textit{clustering:}\\ Grouping data into sets of similiar objects.
	\item \textit{classification:}\\ Assigning Categories to data-objects.
\end{enumerate}

\subsection{Score Function}
\begin{enumerate}
	\item indicates how sensitive a likelihood function $\displaystyle {\mathcal {L}}(\theta ;X)$ is to its parameter $\theta$.
	\item it is defined as:
	\begin{align*}
		u_\theta(x)=\frac{\partial}{\partial \theta}ln \mathcal{L}(\theta \mid x)\,
	\end{align*}
	where $\mathcal{L}$ is a likelihood-function.
\end{enumerate}
\href{https://en.wikipedia.org/wiki/Score_(statistics)}{wikipedia}

\subsection{Likelihood}
\begin{enumerate}
	\item A likelihood function (often simply the likelihood) is a function of the parameters of a statistical model, given specific observed data
	\item Common definitions:
	\begin{enumerate}
		\item \textit{parameterized model}\\
		Given a parameterized family of probability density functions (or probability mass functions in the case of discrete distributions) $x\mapsto f(x\mid\theta)$, where $\theta$ is the parameter, the likelihood function is 
		\begin{align*}
			\theta\mapsto f(x\mid\theta)
		\end{align*}
		, written
		\begin{align*}
		\mathcal{L}(\theta \mid x)=f(x\mid\theta)\,
		\end{align*}
		where $x$ is the observed outcome of an experiment.

		\item \textit{In general}\\
		The likelihood function is this density interpreted as a function of the parameter (possibly a vector), not of the possible outcomes. This provides a likelihood function for any probability model with all distributions, whether discrete, absolutely continuous, a mixture or something else.
	\end{enumerate}
	\item \textit{Log-likelihood:}\\
	It's usually convenient to work with the log-likelihood, especially if multiple, indipendent random variables are involed.
\end{enumerate}
\href{https://en.wikipedia.org/wiki/Likelihood_function}{wikipedia}


\subsection{Maximum-Likelihood Estimator}
\begin{enumerate}
	\item Maximum likelihood estimation (MLE) attempts to find the parameter values that maximize the likelihood function, given the observations.
	\item \textit{frequentist inference:} MLE is one of several methods to get estimates of parameters without using prior distributions.
\end{enumerate}
Some properties:
\begin{enumerate}
	\item \textit{Consistency:} the sequence of MLEs converges in probability to the value being estimated.
	\item \textit{Efficiency:} it achieves the Cramér–Rao lower bound when the sample size tends to infinity. This means that no consistent estimator has lower asymptotic mean squared error than the MLE (or other estimators attaining this bound).
	\item \textit{Second-order efficiency} after correction for bias.
\end{enumerate}
\href{https://en.wikipedia.org/wiki/Divergence_(statistics)}{wikipedia}



\section{Divergences}
Conventions for this section: $P$ and $Q$ are probability measures over a set $X$, and $P$ is absolutely continuous with respect to $Q$. $S$ is a space of all probability distributions with common support.
\subsection{Divergence}
A divergence on $S$ is a function $D: S \times S \rightarrow R$ satisfying
\begin{enumerate}
	\item $D(p || q) \geq 0  \forall p, q \in S$,
	\item $D(p || q) = 0 \Leftrightarrow p = q$
\end{enumerate}
\textit{A divergence is a "sense" of distance between two probability distributions. It's not a metric, but a pre-metric.}\\
\href{https://en.wikipedia.org/wiki/Divergence_(statistics)}{wikipedia}

\subsection{f-Divergence}
\begin{enumerate}
	\item Generalization of whole family of divergences
	\item For a convex function $f$ such that $f(1) = 0$, the f-divergence of $P$ from $Q$ is defined as:\\
	$D_{f}(P\parallel Q)\equiv \int _{{\Omega }}f\left({\frac{dP}{dQ}}\right)\,dQ$
	\item \href{https://en.wikipedia.org/wiki/Divergence_(statistics)}{wikipedia}
\end{enumerate}

\subsection{KL-Divergence}
\begin{enumerate}
	\item The Kullback–Leibler divergence from $Q$ to $P$ is defined as\\
	$D_{\mathrm {KL} }(P\|Q)=\int _{X}\log {\frac {dP}{dQ}}\,dP=D_{t\log t}$.
	\item maxmizing likelihood is equivalent to minimizing $D_{KL}(P(. \vert \theta^{\ast}) \, \Vert \, P(. \vert \theta))$ (the foreward-KL Divergence), where $P(. \vert \theta^{\ast})$ is the true distribution and $P(. \vert \theta)$ is our estimate.
	\item \href{https://en.wikipedia.org/wiki/Kullback–Leibler_divergence}{wikipedia}
	\item TODO: Fisher-Matrix infitesimal relationship
\end{enumerate}

\subsection{Jensen–Shannon divergence}
The Jensen–Shannon divergence from $Q$ to $P$ is defined as
\begin{align*}
	{{\rm {JSD}}}(P\parallel Q)={\frac  {1}{2}}D(P\parallel M)+{\frac  {1}{2}}D(Q\parallel M)
\end{align*}, where $M={\frac  {1}{2}}(P+Q)$\\
\href{https://en.wikipedia.org/wiki/Jensen–Shannon_divergence}{wikipedia}

\subsection{Wasserstein Metric}
\begin{enumerate}
	\item Also often called Earth-Mover's distance (EDM)
	\item It differs from the usual KL-Divergence in that it's based on optimal-transport and not on local probability differences
\end{enumerate}
Let $(M,d)$ be a metric space with every probability measure on $M$ being a Radon measure (a so-called Radon space). For $p\geq 1$, let $\displaystyle P_{p}(M)$ denote the collection of all probability measures $\mu$ on $M$ with finite $\displaystyle p^{\text{th}}$ moment for some $x_{0}$ in $M$.
\subsubsection{Wasserstein Metric - Primal}
The $\displaystyle p^{\text{th}}$ Wasserstein distance between two probability measures $\mu$ and $\nu$ in $\displaystyle P_{p}(M)$ is defined as
\begin{align*}
W_{{p}}(\mu ,\nu ):=\left(\inf _{{\gamma \in \Gamma (\mu ,\nu )}}\int _{{M\times M}}d(x,y)^{{p}}\,{\mathrm  {d}}\gamma (x,y)\right)^{{1/p}}
\end{align*},
where $\displaystyle \Gamma (\mu ,\nu )$ denotes the collection of all measures on $M\times M$ with marginals $\mu$  and $\nu$. ($\Gamma (\mu ,\nu )$ is also called the set of all couplings of $\mu$  and $\nu$ .)
\subsubsection{Wasserstein Metric - Dual}
The following dual representation of $W_1$ is a special case of the duality theorem of Kantorovich and Rubinstein (1958): when $\mu$ and $\nu$ have bounded support,
\begin{align*}
W_{{1}}(\mu ,\nu )=\sup \left\{\left.\int _{{M}}f(x)\,{\mathrm  {d}}(\mu -\nu )(x)\right|{\mbox{continuous }}f:M\to {\mathbb  {R}},{\mathrm  {Lip}}(f)\leq 1\right\}
\end{align*},
where $Lip(f)$ denotes the minimal Lipschitz constant for $f$.\\
If the metric $d$ is bounded by some constant $C$, then $2W_{{1}}(\mu ,\nu )\leq C\rho (\mu ,\nu )$,
and so convergence in the Radon metric (identical to total variation convergence when M is a Polish space) implies convergence in the Wasserstein metric, but not vice versa.\\
This article uses material from the Wikipedia article
\href{https://en.wikipedia.org/wiki/Wasserstein_metric}{Wasserstein metric} which is released under the \href{https://creativecommons.org/licenses/by-sa/3.0/}{Creative Commons Attribution-Share-Alike License 3.0}.\\
Good introductionary blog-post: \href{https://vincentherrmann.github.io/blog/wasserstein/}{wasserstein}.

\section{Information Geometry}
Information Geometry defines a Riemannian Manifold over probability distributions for statistical models.\\
\subsection{Fisher-Rao Metric}
For the parametric family $\mathcal{P} \in \{ \mathcal{P}_\theta | \theta \in \Theta_L\}$ and every $\alpha, \beta \in \mathbb{R}^d$ with their tangent-vectors $\bar{\alpha}=dp_{\theta + t\alpha}/dt|_{t=0}$ and $\bar{\beta}=dp_{\theta + t\beta}/dt|_{t=0}$, we define the inner local product as follows:
\begin{align*}
	<\bar{\alpha}, \bar{\beta}> &:= \int_M\frac{\bar{\alpha}}{p_\theta}\frac{\bar{\beta}}{p_\theta}p_\theta\\
	&=<\alpha, \mathcal{I}(\theta)\beta>\text{,}
\end{align*}
where $\mathcal{I}(\theta)$ is the Fisher-Information Matrix.
\subsection{Natural Gradient}
The natural gradient is the gradient descent induced by the Fisher-Rao geometry of $\{\mathcal{P}_\theta \}$.\\
\href{https://arxiv.org/abs/1711.01530}{paper}