\section{Reinforcement Learning}

\subsection{Bellman Equations}
\newcommand{\qfunc}[2]{Q_\pi(#1, #2)}
\newcommand{\vfunc}[1]{v_\pi (#1)}
\newcommand{\policy}[2]{\pi (#1 | #2)}

TODO backup diagramms

State Value Function 
\begin{align}
	\vfunc{s} = \sum_{a \in A} \policy{s}{a} \qfunc{s}{a}
\end{align}
Action Value Function 
\begin{align}
	\qfunc{s}{a} = r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \vfunc{s'}
\end{align}
State Value Function recursive
\begin{align}
	\vfunc{s} = \sum_{a \in A} \policy{s}{a} (r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \vfunc{s'})
\end{align}
Action Value Function recursive
\begin{align}
	\qfunc{s}{a} = r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \sum_{a \in A} \policy{a'}{s'}\qfunc{s'}{a'}
\end{align}
Optimal State Value Function
\renewcommand{\qfunc}[2]{Q_{*}(#1, #2)}
\renewcommand{\vfunc}[1]{v_{*} (#1)}
\begin{align}
	\vfunc{s} = \max_a \qfunc{s}{a}
\end{align}
Optimal Action State Value Function
\begin{align}
	\qfunc{s}{a} = r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \vfunc{s'}
\end{align}
Optimal State Value Function recursive
\begin{align}
	\vfunc{s} = \max\limits_a r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \vfunc{s'}
\end{align}
Optimal Action State Value Function recursive
\begin{align}
	\qfunc{a}{s} = r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \max\limits_{a'} \qfunc{s'}{a'}
\end{align}


\subsection{Advantage Function}

TODO


\subsection{Policy, Policy Gradient}

Policy: Distribution over actions given states
\renewcommand{\policy}[2]{\pi_\theta (#1 | #2)}
\begin{align}
	\policy{a}{s} = P(a | s)
\end{align}
Policy Gradient
\begin{align}
	\nabla_\theta \policy{s}{a} = \policy{s}{a} \nabla_\theta \log \policy{s}{a}
\end{align}
Note: this is valid for all probability distributions (the policy is a distribution over actions given states). The gradient term on the right hand side is called score function. The derivation basically uses the "log-trick".